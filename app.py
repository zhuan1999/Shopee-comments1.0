import streamlit as st
import pandas as pd
import requests
import time
import json
import re
from datetime import datetime
from io import BytesIO
import plotly.express as px
import plotly.graph_objects as go
from typing import List, Dict, Optional
import base64

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Shopeeè¯„è®ºçˆ¬å–åˆ†æå·¥å…·",
    page_icon="ğŸ›ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #ee4d2d;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stButton button {
        background-color: #ee4d2d;
        color: white;
        font-weight: bold;
        border: none;
        padding: 0.5rem 2rem;
        border-radius: 5px;
        transition: all 0.3s;
    }
    .stButton button:hover {
        background-color: #d83b1f;
        transform: scale(1.05);
    }
    .success-box {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #d1ecf1;
        color: #0c5460;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .review-card {
        background-color: #f8f9fa;
        border-left: 4px solid #ee4d2d;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0 5px 5px 0;
    }
    .rating-badge {
        display: inline-block;
        background-color: #ffc107;
        color: #000;
        padding: 0.25rem 0.5rem;
        border-radius: 3px;
        font-weight: bold;
        margin-right: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

# æ ‡é¢˜
st.markdown('<h1 class="main-header">ğŸ›ï¸ Shopeeè¯„è®ºçˆ¬å–åˆ†æå·¥å…·</h1>', unsafe_allow_html=True)
st.markdown('<p class="sub-header">é«˜æ•ˆæå–ã€åˆ†æShopeeå•†å“è¯„è®ºæ•°æ®</p>', unsafe_allow_html=True)

class ShopeeReviewScraper:
    """Shopeeè¯„è®ºçˆ¬å–å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json',
            'Accept-Language': 'en-US,en;q=0.9',
            'Referer': 'https://shopee.co.id/',
        })
    
    def extract_ids_from_url(self, url: str) -> tuple:
        """ä»URLæå–å•†å“IDå’Œåº—é“ºID"""
        try:
            # æ–¹æ³•1ï¼šä»URLæ¨¡å¼æå–
            patterns = [
                r'i\.(\d+)\.(\d+)',  # æ ‡å‡†Shopee URLæ¨¡å¼
                r'item/(\d+)/(\d+)',  # å¦ä¸€ç§æ¨¡å¼
                r'shopid=(\d+)&itemid=(\d+)',  # å‚æ•°æ¨¡å¼
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    shop_id, item_id = match.groups()
                    return shop_id, item_id
            
            # æ–¹æ³•2ï¼šå°è¯•ä»HTMLé¡µé¢æå–ï¼ˆå¦‚æœæä¾›äº†å®Œæ•´URLï¼‰
            if url.startswith('http'):
                response = self.session.get(url, timeout=10)
                # æŸ¥æ‰¾å•†å“IDå’Œåº—é“ºID
                html = response.text
                shop_id_match = re.search(r'"shopid"\s*:\s*(\d+)', html)
                item_id_match = re.search(r'"itemid"\s*:\s*(\d+)', html)
                
                if shop_id_match and item_id_match:
                    return shop_id_match.group(1), item_id_match.group(1)
            
            return None, None
            
        except Exception as e:
            st.warning(f"URLè§£æå¤±è´¥: {str(e)}")
            return None, None
    
    def fetch_reviews_api(self, shop_id: str, item_id: str, limit: int = 100) -> List[Dict]:
        """é€šè¿‡APIè·å–è¯„è®ºæ•°æ®"""
        all_reviews = []
        
        try:
            # Shopeeè¯„è®ºAPIï¼ˆå°å°¼ç«™ï¼‰
            base_url = "https://shopee.co.id/api/v2/item/get_ratings"
            
            offset = 0
            batch_size = 20  # æ¯é¡µ20æ¡
            
            with st.spinner("æ­£åœ¨çˆ¬å–è¯„è®ºæ•°æ®..."):
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                while offset < limit:
                    params = {
                        'itemid': item_id,
                        'shopid': shop_id,
                        'offset': offset,
                        'limit': batch_size,
                        'type': 0,  # æ‰€æœ‰è¯„è®º
                        'filter': 0,  # æ‰€æœ‰ç±»å‹
                        'flag': 1
                    }
                    
                    try:
                        response = self.session.get(base_url, params=params, timeout=15)
                        
                        if response.status_code == 200:
                            data = response.json()
                            
                            if data.get('error'):
                                st.error(f"APIé”™è¯¯: {data.get('error', 'æœªçŸ¥é”™è¯¯')}")
                                break
                            
                            ratings = data.get('data', {}).get('ratings', [])
                            
                            if not ratings:
                                break  # æ²¡æœ‰æ›´å¤šæ•°æ®
                            
                            for rating in ratings:
                                review = self.parse_review(rating)
                                if review:
                                    all_reviews.append(review)
                            
                            # æ›´æ–°è¿›åº¦
                            progress = min(100, int((offset / limit) * 100))
                            progress_bar.progress(progress)
                            status_text.text(f"å·²è·å– {len(all_reviews)} æ¡è¯„è®º...")
                            
                            offset += batch_size
                            
                            # å»¶è¿Ÿé¿å…è¢«å°
                            time.sleep(0.5)
                            
                        else:
                            st.warning(f"è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                            break
                            
                    except Exception as e:
                        st.error(f"è¯·æ±‚å‡ºé”™: {str(e)}")
                        break
                
                progress_bar.progress(100)
                status_text.text(f"å®Œæˆï¼å…±è·å– {len(all_reviews)} æ¡è¯„è®º")
                
        except Exception as e:
            st.error(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}")
        
        return all_reviews
    
    def fetch_reviews_selenium(self, url: str, max_reviews: int = 100) -> List[Dict]:
        """ä½¿ç”¨Seleniumæ¨¡æ‹Ÿæµè§ˆå™¨è·å–è¯„è®ºï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            # è¿™é‡Œéœ€è¦å®‰è£…seleniumå’Œwebdriver
            from selenium import webdriver
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.webdriver.chrome.options import Options
            
            st.info("æ­£åœ¨å¯åŠ¨æµè§ˆå™¨æ¨¡æ‹Ÿ...")
            
            # è®¾ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            driver = webdriver.Chrome(options=chrome_options)
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            driver.get(url)
            time.sleep(3)
            
            reviews = []
            
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´é€‰æ‹©å™¨
            # ç”±äºé¡µé¢ç»“æ„å¯èƒ½å˜åŒ–ï¼Œè¿™åªæ˜¯ä¸€ä¸ªç¤ºä¾‹
            try:
                review_elements = driver.find_elements(By.CSS_SELECTOR, 'div[class*="product-review"]')
                
                for element in review_elements[:max_reviews]:
                    try:
                        # è§£æè¯„è®ºå…ƒç´ 
                        review_data = self.parse_review_element(element)
                        if review_data:
                            reviews.append(review_data)
                    except:
                        continue
                        
            except Exception as e:
                st.warning(f"è§£æé¡µé¢å¤±è´¥: {str(e)}")
            
            driver.quit()
            return reviews
            
        except ImportError:
            st.error("éœ€è¦å®‰è£…selenium: pip install selenium")
            return []
        except Exception as e:
            st.error(f"Seleniumçˆ¬å–å¤±è´¥: {str(e)}")
            return []
    
    def parse_review(self, rating_data: Dict) -> Optional[Dict]:
        """è§£æAPIè¿”å›çš„è¯„è®ºæ•°æ®"""
        try:
            # æå–ç”¨æˆ·ä¿¡æ¯
            username = rating_data.get('author_username', '')
            if not username or username == 'null':
                username = rating_data.get('author_portrait', '').split('/')[-1].split('.')[0]
            
            # å¤„ç†åŒ¿åç”¨æˆ·
            if not username or len(username) < 2:
                username = f"ç”¨æˆ·_{hash(rating_data.get('cmtid', '')) % 10000:04d}"
            
            # è¯„åˆ†
            rating = rating_data.get('rating_star', 0)
            
            # è¯„è®ºå†…å®¹
            comment = rating_data.get('comment', '')
            if not comment or comment == 'null':
                comment = rating_data.get('detailed_rating', [{}])[0].get('comment', '') if rating_data.get('detailed_rating') else ''
            
            # æ—¶é—´æˆ³è½¬æ¢
            ctime = rating_data.get('ctime', 0)
            if ctime:
                try:
                    review_time = datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M')
                except:
                    review_time = str(ctime)
            else:
                review_time = 'æœªçŸ¥æ—¶é—´'
            
            # äº§å“å˜ä½“
            product_items = rating_data.get('product_items', [{}])
            variation = product_items[0].get('model_name', '') if product_items else ''
            
            # å–å®¶å›å¤
            seller_response = ''
            if rating_data.get('seller_reply'):
                seller_response = rating_data['seller_reply'].get('comment', '')
            
            # ç‚¹èµæ•°
            like_count = rating_data.get('like_count', 0)
            
            return {
                'username': username,
                'time': review_time,
                'rating': rating,
                'comment': comment,
                'variation': variation,
                'seller_response': seller_response,
                'like_count': like_count,
                'images_count': len(rating_data.get('images', [])),
                'source': 'api'
            }
            
        except Exception as e:
            st.warning(f"è§£æè¯„è®ºå¤±è´¥: {str(e)}")
            return None
    
    def parse_review_element(self, element) -> Optional[Dict]:
        """è§£æSeleniumè·å–çš„è¯„è®ºå…ƒç´ """
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
            # ç”±äºé¡µé¢ç»“æ„å¯èƒ½å˜åŒ–ï¼Œè¿™åªæ˜¯ä¸€ä¸ªç¤ºä¾‹è§£æé€»è¾‘
            text = element.text
            
            # æå–ç”¨æˆ·åï¼ˆé€šå¸¸ä»¥*å·éšè—éƒ¨åˆ†å­—ç¬¦ï¼‰
            username_match = re.search(r'^([^*\n]+)', text)
            username = username_match.group(1).strip() if username_match else 'åŒ¿åç”¨æˆ·'
            
            # æå–è¯„åˆ†ï¼ˆé€šè¿‡â˜…ç¬¦å·æ•°é‡ï¼‰
            stars = text.count('â˜…')
            rating = stars if 1 <= stars <= 5 else 5
            
            # æå–æ—¥æœŸ
            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', text)
            review_time = date_match.group(1) if date_match else 'æœªçŸ¥æ—¶é—´'
            
            # æå–è¯„è®ºå†…å®¹ï¼ˆç®€åŒ–æå–ï¼‰
            lines = text.split('\n')
            comment_lines = []
            in_comment = False
            
            for line in lines:
                if line.strip() and not line.startswith(username) and not re.match(r'\d{4}-\d{2}-\d{2}', line):
                    if 'Variation:' not in line and 'Seller' not in line:
                        comment_lines.append(line.strip())
            
            comment = ' '.join(comment_lines[:3])  # åªå–å‰3è¡Œ
            
            return {
                'username': username,
                'time': review_time,
                'rating': rating,
                'comment': comment[:200],  # é™åˆ¶é•¿åº¦
                'variation': '',
                'seller_response': '',
                'like_count': 0,
                'images_count': 0,
                'source': 'selenium'
            }
            
        except Exception as e:
            return None
    
    def analyze_reviews(self, reviews_df):
        """åˆ†æè¯„è®ºæ•°æ®"""
        analysis = {}
        
        if reviews_df.empty:
            return analysis
        
        # åŸºæœ¬ç»Ÿè®¡
        analysis['total_reviews'] = len(reviews_df)
        analysis['avg_rating'] = reviews_df['rating'].mean()
        
        # è¯„åˆ†åˆ†å¸ƒ
        rating_counts = reviews_df['rating'].value_counts().sort_index()
        analysis['rating_distribution'] = rating_counts
        
        # æ—¶é—´åˆ†æï¼ˆå¦‚æœæœ‰æ—¶é—´æ•°æ®ï¼‰
        if 'time' in reviews_df.columns and pd.api.types.is_datetime64_any_dtype(reviews_df['time']):
            reviews_df['date'] = reviews_df['time'].dt.date
            daily_counts = reviews_df['date'].value_counts().sort_index()
            analysis['daily_trend'] = daily_counts
        
        # è¯„è®ºé•¿åº¦åˆ†æ
        reviews_df['comment_length'] = reviews_df['comment'].apply(len)
        analysis['avg_comment_length'] = reviews_df['comment_length'].mean()
        
        # æƒ…æ„Ÿå…³é”®è¯ï¼ˆç®€å•ç‰ˆï¼‰
        positive_words = ['bagus', 'baik', 'mantap', 'puas', 'recommended', 'suka', 'senang', 'glowing', 'cerah']
        negative_words = ['jelek', 'buruk', 'kecewa', 'tidak', 'gagal', 'rusak', 'palsu']
        
        reviews_df['positive_score'] = reviews_df['comment'].apply(
            lambda x: sum(1 for word in positive_words if word.lower() in x.lower())
        )
        reviews_df['negative_score'] = reviews_df['comment'].apply(
            lambda x: sum(1 for word in negative_words if word.lower() in x.lower())
        )
        
        analysis['positive_count'] = (reviews_df['positive_score'] > 0).sum()
        analysis['negative_count'] = (reviews_df['negative_score'] > 0).sum()
        
        return analysis

def main():
    """ä¸»å‡½æ•°"""
    
    # åˆå§‹åŒ–çˆ¬è™«
    scraper = ShopeeReviewScraper()
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Shopee.svg/320px-Shopee.svg.png", 
                 width=150, caption="Shopee Indonesia")
        
        st.markdown("### âš™ï¸ é…ç½®é€‰é¡¹")
        
        # è¾“å…¥æ–¹å¼é€‰æ‹©
        input_method = st.radio("è¾“å…¥æ–¹å¼", ["å•†å“URL", "æ‰‹åŠ¨è¾“å…¥ID"])
        
        if input_method == "å•†å“URL":
            product_url = st.text_input(
                "å•†å“é“¾æ¥",
                placeholder="https://shopee.co.id/...",
                help="ç²˜è´´å®Œæ•´çš„Shopeeå•†å“é“¾æ¥"
            )
            shop_id, item_id = None, None
            
            if product_url:
                with st.spinner("æ­£åœ¨è§£æURL..."):
                    shop_id, item_id = scraper.extract_ids_from_url(product_url)
                
                if shop_id and item_id:
                    st.success(f"è§£ææˆåŠŸï¼")
                    st.info(f"åº—é“ºID: `{shop_id}`")
                    st.info(f"å•†å“ID: `{item_id}`")
                else:
                    st.warning("æ— æ³•ä»URLè§£æIDï¼Œè¯·æ£€æŸ¥é“¾æ¥æ ¼å¼")
        else:
            shop_id = st.text_input("åº—é“ºID", placeholder="å¦‚ï¼š809769142")
            item_id = st.text_input("å•†å“ID", placeholder="å¦‚ï¼š42800295602")
        
        # çˆ¬å–è®¾ç½®
        st.markdown("### ğŸ“Š çˆ¬å–è®¾ç½®")
        max_reviews = st.slider("æœ€å¤§è¯„è®ºæ•°", 10, 1000, 100, 10)
        
        use_api = st.checkbox("ä½¿ç”¨APIçˆ¬å–ï¼ˆæ¨èï¼‰", value=True)
        use_selenium = st.checkbox("ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿï¼ˆå¤‡ç”¨ï¼‰", value=False)
        
        if use_selenium:
            st.warning("Seleniuméœ€è¦é¢å¤–å®‰è£…ï¼Œé€Ÿåº¦è¾ƒæ…¢")
        
        st.markdown("### ğŸ’¾ å¯¼å‡ºé€‰é¡¹")
        export_format = st.multiselect(
            "å¯¼å‡ºæ ¼å¼",
            ["CSV", "Excel", "JSON"],
            default=["CSV"]
        )
        
        st.markdown("---")
        
        if st.button("ğŸš€ å¼€å§‹çˆ¬å–", type="primary", use_container_width=True):
            st.session_state.start_scraping = True
        else:
            st.session_state.start_scraping = False
    
    # ä¸»ç•Œé¢
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### ğŸ“ è¾“å…¥å•†å“ä¿¡æ¯")
        
        # å¦‚æœæœªä»ä¾§è¾¹æ è·å–ï¼Œåœ¨è¿™é‡Œä¹Ÿå¯ä»¥è¾“å…¥
        if input_method == "å•†å“URL" and not product_url:
            product_url = st.text_input("åœ¨è¿™é‡Œè¾“å…¥å•†å“é“¾æ¥ï¼š", key="main_url")
            if product_url:
                shop_id, item_id = scraper.extract_ids_from_url(product_url)
        
        if not shop_id or not item_id:
            st.info("è¯·åœ¨ä¾§è¾¹æ è¾“å…¥å•†å“ä¿¡æ¯")
            
    with col2:
        st.markdown("### ğŸ“Š æ•°æ®é¢„è§ˆ")
        if 'reviews_df' in st.session_state and not st.session_state.reviews_df.empty:
            df = st.session_state.reviews_df
            st.metric("æ€»è¯„è®ºæ•°", len(df))
            st.metric("å¹³å‡è¯„åˆ†", f"{df['rating'].mean():.1f} â­")
            st.metric("æœ‰å›¾ç‰‡çš„è¯„è®º", f"{df[df['images_count'] > 0].shape[0]} æ¡")
    
    # çˆ¬å–æŒ‰é’®è§¦å‘
    if st.session_state.get('start_scraping', False) and shop_id and item_id:
        st.markdown("---")
        st.markdown("### ğŸ” æ­£åœ¨çˆ¬å–è¯„è®º...")
        
        # æ¸…ç©ºä¹‹å‰çš„ç¼“å­˜
        if 'reviews_df' in st.session_state:
            del st.session_state.reviews_df
        
        # çˆ¬å–è¯„è®º
        reviews = []
        
        if use_api:
            reviews = scraper.fetch_reviews_api(shop_id, item_id, max_reviews)
        
        if use_selenium and len(reviews) < max_reviews:
            if product_url:
                selenium_reviews = scraper.fetch_reviews_selenium(product_url, max_reviews - len(reviews))
                reviews.extend(selenium_reviews)
        
        if reviews:
            # è½¬æ¢ä¸ºDataFrame
            reviews_df = pd.DataFrame(reviews)
            
            # æ—¶é—´åˆ—è½¬æ¢
            if 'time' in reviews_df.columns:
                reviews_df['time'] = pd.to_datetime(reviews_df['time'], errors='coerce')
            
            # ä¿å­˜åˆ°session
            st.session_state.reviews_df = reviews_df
            
            # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯
            st.success(f"âœ… æˆåŠŸçˆ¬å– {len(reviews_df)} æ¡è¯„è®ºï¼")
            
            # æ˜¾ç¤ºæ•°æ®
            st.markdown("### ğŸ“‹ è¯„è®ºæ•°æ®è¡¨")
            st.dataframe(
                reviews_df[['username', 'time', 'rating', 'comment', 'like_count']].head(20),
                use_container_width=True,
                hide_index=True
            )
            
            # æ•°æ®åˆ†æ
            st.markdown("### ğŸ“ˆ æ•°æ®åˆ†æ")
            
            # è¯„åˆ†åˆ†å¸ƒå›¾è¡¨
            col1, col2 = st.columns(2)
            
            with col1:
                fig1 = go.Figure(data=[
                    go.Pie(
                        labels=[f'{i}æ˜Ÿ' for i in range(1, 6)],
                        values=[(reviews_df['rating'] == i).sum() for i in range(1, 6)],
                        hole=.3,
                        marker_colors=['#ff6b6b', '#ffa726', '#ffd166', '#06d6a0', '#118ab2']
                    )
                ])
                fig1.update_layout(title_text="è¯„åˆ†åˆ†å¸ƒ", height=300)
                st.plotly_chart(fig1, use_container_width=True)
            
            with col2:
                # è¯„è®ºé•¿åº¦åˆ†å¸ƒ
                reviews_df['comment_length'] = reviews_df['comment'].apply(len)
                fig2 = px.histogram(
                    reviews_df, 
                    x='comment_length',
                    nbins=20,
                    title="è¯„è®ºé•¿åº¦åˆ†å¸ƒ",
                    labels={'comment_length': 'è¯„è®ºå­—æ•°'}
                )
                fig2.update_layout(height=300)
                st.plotly_chart(fig2, use_container_width=True)
            
            # æ—¶é—´è¶‹åŠ¿ï¼ˆå¦‚æœæœ‰æ—¶é—´æ•°æ®ï¼‰
            if 'time' in reviews_df.columns and not reviews_df['time'].isna().all():
                reviews_df['date'] = reviews_df['time'].dt.date
                daily_counts = reviews_df['date'].value_counts().sort_index()
                
                fig3 = px.line(
                    x=daily_counts.index,
                    y=daily_counts.values,
                    title="æ¯æ—¥è¯„è®ºè¶‹åŠ¿",
                    labels={'x': 'æ—¥æœŸ', 'y': 'è¯„è®ºæ•°'}
                )
                st.plotly_chart(fig3, use_container_width=True)
            
            # å¯¼å‡ºåŠŸèƒ½
            st.markdown("### ğŸ’¾ å¯¼å‡ºæ•°æ®")
            
            col1, col2, col3 = st.columns(3)
            
            # CSVå¯¼å‡º
            if "CSV" in export_format:
                with col1:
                    csv = reviews_df.to_csv(index=False).encode('utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½CSV",
                        data=csv,
                        file_name=f"shopee_reviews_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv",
                        help="ä¸‹è½½ä¸ºCSVæ ¼å¼ï¼Œå¯ç”¨Excelæ‰“å¼€"
                    )
            
            # Excelå¯¼å‡º
            if "Excel" in export_format:
                with col2:
                    # ä½¿ç”¨BytesIOåˆ›å»ºExcelæ–‡ä»¶
                    output = BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        reviews_df.to_excel(writer, index=False, sheet_name='è¯„è®ºæ•°æ®')
                        
                        # æ·»åŠ æ±‡æ€»è¡¨
                        summary_df = pd.DataFrame({
                            'ç»Ÿè®¡é¡¹': ['æ€»è¯„è®ºæ•°', 'å¹³å‡è¯„åˆ†', 'æœ€é•¿è¯„è®º', 'æœ€çŸ­è¯„è®º'],
                            'å€¼': [
                                len(reviews_df),
                                f"{reviews_df['rating'].mean():.2f}",
                                reviews_df['comment'].apply(len).max(),
                                reviews_df['comment'].apply(len).min()
                            ]
                        })
                        summary_df.to_excel(writer, index=False, sheet_name='æ•°æ®æ±‡æ€»')
                    
                    excel_data = output.getvalue()
                    
                    st.download_button(
                        label="ğŸ“Š ä¸‹è½½Excel",
                        data=excel_data,
                        file_name=f"shopee_reviews_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        help="ä¸‹è½½ä¸ºExcelæ ¼å¼ï¼ŒåŒ…å«æ•°æ®æ±‡æ€»"
                    )
            
            # JSONå¯¼å‡º
            if "JSON" in export_format:
                with col3:
                    json_str = reviews_df.to_json(orient='records', force_ascii=False, indent=2)
                    st.download_button(
                        label="ğŸ“„ ä¸‹è½½JSON",
                        data=json_str,
                        file_name=f"shopee_reviews_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json",
                        help="ä¸‹è½½ä¸ºJSONæ ¼å¼ï¼Œä¾¿äºç¨‹åºå¤„ç†"
                    )
            
            # æ˜¾ç¤ºåŸå§‹æ•°æ®é€‰é¡¹
            with st.expander("æŸ¥çœ‹åŸå§‹æ•°æ®"):
                st.json(reviews_df.head(10).to_dict(orient='records'))
                
        else:
            st.error("æœªèƒ½è·å–åˆ°è¯„è®ºæ•°æ®ï¼Œè¯·å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š")
            st.markdown("""
            1. æ£€æŸ¥å•†å“é“¾æ¥æ˜¯å¦æ­£ç¡®
            2. å°è¯•ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿæ¨¡å¼
            3. ç¡®ä¿å•†å“æœ‰è¯„è®º
            4. å¯èƒ½æ˜¯Shopeeåçˆ¬è™«æœºåˆ¶ï¼Œè¯·ç¨åé‡è¯•
            """)
    
    # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œæ˜¾ç¤ºç¤ºä¾‹
    elif 'reviews_df' not in st.session_state:
        st.markdown("---")
        st.markdown("### ğŸ¯ ä½¿ç”¨è¯´æ˜")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("#### 1ï¸âƒ£ è¾“å…¥å•†å“ä¿¡æ¯")
            st.markdown("""
            - ç²˜è´´å•†å“å®Œæ•´é“¾æ¥
            - æˆ–æ‰‹åŠ¨è¾“å…¥åº—é“ºIDå’Œå•†å“ID
            """)
        
        with col2:
            st.markdown("#### 2ï¸âƒ£ é…ç½®çˆ¬å–é€‰é¡¹")
            st.markdown("""
            - è®¾ç½®æœ€å¤§è¯„è®ºæ•°
            - é€‰æ‹©çˆ¬å–æ–¹æ³•
            - é€‰æ‹©å¯¼å‡ºæ ¼å¼
            """)
        
        with col3:
            st.markdown("#### 3ï¸âƒ£ å¼€å§‹çˆ¬å–åˆ†æ")
            st.markdown("""
            - ç‚¹å‡»å¼€å§‹çˆ¬å–
            - æŸ¥çœ‹æ•°æ®åˆ†æ
            - å¯¼å‡ºæ‰€éœ€æ ¼å¼
            """)
        
        # ç¤ºä¾‹æ•°æ®
        st.markdown("### ğŸ“Š ç¤ºä¾‹æ•°æ®é¢„è§ˆ")
        example_data = pd.DataFrame({
            'username': ['ç”¨æˆ·_1234', 'ç”¨æˆ·_5678', 'ç”¨æˆ·_9101'],
            'time': ['2025-01-15', '2025-01-14', '2025-01-13'],
            'rating': [5, 4, 3],
            'comment': ['äº§å“è´¨é‡å¾ˆå¥½ï¼Œå¾ˆå–œæ¬¢ï¼', 'å‘è´§é€Ÿåº¦æœ‰ç‚¹æ…¢ï¼Œä½†äº§å“ä¸é”™', 'ä¸€èˆ¬èˆ¬ï¼Œæ²¡æœ‰æƒ³è±¡ä¸­å¥½'],
            'like_count': [10, 5, 2]
        })
        st.dataframe(example_data, use_container_width=True, hide_index=True)

if __name__ == "__main__":
    main()
